#!/usr/bin/env python
#
# plaindumb - a cleaner, less surprising syntax than markdown
#
#
# Default to html to stdout:
#
# $ plaindumb input.txt > output.html
#
#
# Or use file suffix of second argument to determine output format:
#
# $ plaindumb input.txt output.html
# $ plaindumb input.txt output.rst
#

import os
import sys
import re

class message:
  def msg(self, s):
    """Print a message to the screen (stdout)."""
    if not self.quiet:
      print "--- " + str(s)
      sys.stdout.flush()

  def yay(self, s):
    """Print a success message to the screen (stdout)."""
    if not self.quiet:
      print ":-] " + str(s)
      sys.stdout.flush()

  def err(self, s):
    """Print a failure message to the screen (stderr)."""
    sys.stderr.write("!!! "+str(s)+"\n")

  def die(self, s):
    """Print a failure message to the screen (stderr) and then halt."""
    self.err(s)
    sys.exit(1)

  def dbg(self, s):
    """Print a message to the screen (stdout) for debugging only."""
    if self.debug:
      print "DBG " + str(s)
      sys.stdout.flush()



class tokenizer:

  tokens = {"nl"   : "{PD-TOKEN-NL}"          ,"nlp"  : "{PD-TOKEN-NLP}"
           ,"h1_s" : "{PD-S-TOKEN-HEADER-1}"  ,"h1_e" : "{PD-E-TOKEN-HEADER-1}"
           ,"h2_s" : "{PD-S-TOKEN-HEADER-2}"  ,"h2_e" : "{PD-E-TOKEN-HEADER-2}"
           ,"h3_s" : "{PD-S-TOKEN-HEADER-3}"  ,"h3_e" : "{PD-E-TOKEN-HEADER-3}"
           ,"pa_s" : "{PD-S-TOKEN-PARAGRAPH}" ,"pa_e" : "{PD-E-TOKEN-PARAGRAPH}"
           ,"pr_s" : "{PD-S-TOKEN-PRE}"       ,"pr_e" : "{PD-E-TOKEN-PRE}"
           ,"ol_s" : "{PD-S-TOKEN-OL}"        ,"ol_e" : "{PD-E-TOKEN-OL}"
           ,"ul_s" : "{PD-S-TOKEN-UL}"        ,"ul_e" : "{PD-E-TOKEN-UL}"
           ,"li"   : "{PD-TOKEN-LI}"          ,"lin"  : "{PD-TOKEN-LIN}"
           ,"a_s"  : "{PD-S-TOKEN-A}"         ,"a_m"  : "{PD-M-TOKEN-A}" ,"a_e"  : "{PD-E-TOKEN-A}"
           ,"b_s"  : "{PD-S-TOKEN-B}"         ,"b_e"  : "{PD-E-TOKEN-B}"
           ,"u_s"  : "{PD-S-TOKEN-U}"         ,"u_e"  : "{PD-E-TOKEN-U}"
           ,"i_s"  : "{PD-S-TOKEN-I}"         ,"i_e"  : "{PD-E-TOKEN-I}"
           ,"star": "{PD-TOKEN-STAR}"
           ,"slash": "{PD-TOKEN-SLASH}"
           ,"under": "{PD-TOKEN-UNDERSCORE}"
           ,"lt"   : "{PD-TOKEN-LESSTHAN}" ,"gt"   : "{PD-TOKEN-GREATERTHAN}"
           }

  table_of_contents = {}

  def __init__(self):
    pass

  def tokenize(self, lines):
    m = message()
    t = self.tokens

    # prepend new line characters, this makes the subsequent regexes simpler
    lines = "\n\n"+lines

    # Handle the Do-Not-Touch Content markers eg: >>>content<<<
    # DOTALL means that . matches newlines
    if re.search('>>>(.*?)<<<', lines, re.DOTALL):
      bits = re.split('(>>>|<<<)',lines,re.DOTALL)
      go = False
      lines = ''
      for bit in bits:
        if go:
          bit = bit.replace('\n',t['nlp']) # permanent newlines
          go = False
        if bit == '>>>':
          go = True
        if bit == '<<<':
          go = False
        lines += bit

    lines = lines.replace("\n",t['nl'])
    clumps = lines.split(t['nl']+t['nl'])
    stack = []

    for line in clumps:
      doNothing = False

      # strip leading and trailing nl from clump
      line = re.sub('^'+t['nl'],"",line)
      line = re.sub(t['nl']+'$',"",line)

      # Text delimited by >>> and <<< is unaltered
      if re.match('>>>(.*?)<<<', line):
        doNothing = True
        line = line.replace('>>>','').replace('<<<','')

      # <pre>
      elif len(line)>0 and line[0] == ' ':
        start, end = t['pr_s'], t['pr_e']
        line = line.replace(t['nl'],"\n")

      # <ol>
      elif re.match('\s*[0-9]+', line): 
        start, end = t['ol_s'], t['ol_e']
        line = t['lin'].join(re.split(t['nl']+'\s*[0-9]+[\.|\)]\s*',t['nl']+line))

      # <ul>
      elif re.match('\s*[\*|\-]+ ', line): 
        start, end = t['ul_s'], t['ul_e']
        line = t['li'].join(re.split(t['nl']+'\s*[\*|\-]+\s*',t['nl']+line))

      # <h1>
      elif re.match('([^#]+)'+t['nl']+'[#]+', line): 
        start, end = t['h1_s'], t['h1_e']
        line = re.sub('([^#]+)'+t['nl']+'[#]+',self.add_h1_to_table_of_contents, line).replace(t['nl'], "")

      # <h2>
      elif re.match('([^=]+)'+t['nl']+'[=]+', line): 
        start, end = t['h2_s'], t['h2_e']
        line = re.sub('([^=]+)'+t['nl']+'[=]+',self.add_h2_to_table_of_contents, line).replace(t['nl'], "")

      # <h3>
      elif re.match('([^-]+)'+t['nl']+'[-]+', line): 
        start, end = t['h3_s'], t['h3_e']
        line = re.sub('([^-]+)'+t['nl']+'[-]+',self.add_h3_to_table_of_contents, line).replace(t['nl'], "")

      # <p>
      elif line:
        start = t['pa_s']
        end = t['pa_e']
        line = line.replace(t['nl']," ")

      else:
        start = ''
        end= ''

      if not doNothing:
        line = line.replace('<',t['lt']).replace('>',t['gt'])
        line = self.inline_decoration(line)

      stack.append(start+line+end)

      # load up the table of contents
      slots = [0,0,0,0,0]
      for i, v in self.table_of_contents.items():
        j = 1
        num = ''
        dot = ''
        n = v['level']
        slots[n] += 1
        slots[n+1] = 0
        while j <= n:
          num += dot + str(slots[j])
          dot = '.'
          j += 1
        self.table_of_contents[i]['prefix'] = num

    return "".join(stack)
      

  def inline_decoration(self, line):
    t = self.tokens

    # <a> links
    line = re.sub('(^.*)(https?://[^ ]+)( \(.*\))?(.*)', self.encode_links, line)

    # <b> *bold*

    go = False
    if re.search('variables',line):
      #print line
      go = True

    #line = re.sub('(\A|\s|\W){1}\*(.*?)\*(\W|\s|\Z){1}','\\1'+t['b_s']+'\\2'+t['b_e']+'\\3',line)

    line = re.sub('(\A|\s|\W){1}\*(.*?\*)','\\1'+t['b_s']+'\\2',line)
    line = re.sub('(.*?)\*(\W|\s|\Z){1}','\\1'+t['b_e']+'\\2',line)

    #if go:
      #print line
      #sys.exit()

    linestartcharacters = '"\'([`~'
    lineendcharacters = '"\')]`,.!?:;~'

    # <u> _underlined_
    line = re.sub('(\A|\s|\W){1}_(.*?)_(\W|\s|\Z){1}',  '\\1'+t['u_s']+'\\2'+t['u_e']+'\\3',line)

    # <i> /italic/   but/not/this
    line = re.sub('(\A|\s){1}\/(.*?)\/(\s|\Z){1}','\\1'+t['i_s']+'\\2'+t['i_e']+'\\3',line)


    return line

  def encode_links(self, m):
    t = self.tokens

    # the label for the link might be text or the url itself
    label = m.group(3) or m.group(2)

    # nuke the brackets
    label = re.sub('^\s?\((.*)\)$','\\1',label)

    # replace /, _* with tokens, because they are special characters
    label = str(label.replace('/',t['slash']).replace('_',t['under']).replace('*',t['star']))
    link = str(m.group(2).replace('/',t['slash']).replace('_',t['under']).replace('*',t['star']))

    # what a clusterfuck
    s = str(m.group(1))+t['a_s']+link+t['a_m']+label+t['a_e']
    if m.group(4):
      s += str(m.group(4))
    return s

  def add_h1_to_table_of_contents(self,m):
    #self.table_of_contents[len(self.table_of_contents)] = {'prefix':'', 'label':m.group(1), 'level':1}
    #return '{H-ID-'+str(len(self.table_of_contents)-1)+'} '+ m.group(1)
    return m.group(1)

  def add_h2_to_table_of_contents(self,m):
    self.table_of_contents[len(self.table_of_contents)] = {'prefix':'', 'label':m.group(1), 'level':1}
    return '{H-ID-'+str(len(self.table_of_contents)-1)+'} '+ m.group(1)

  def add_h3_to_table_of_contents(self,m):
    self.table_of_contents[len(self.table_of_contents)] = {'prefix':'', 'label':m.group(1), 'level':2}
    return '{H-ID-'+str(len(self.table_of_contents)-1)+'} '+ m.group(1)



class htmlConverter():

  translation = {"nl"   : "\n<br />"  ,"nlp"  : "\n"
                ,"h1_s" : "\n<h1>"    ,"h1_e" : "</h1>"
                ,"h2_s" : "\n<h2>"    ,"h2_e" : "</h2>"
                ,"h3_s" : "\n<h3>"    ,"h3_e" : "</h3>"
                ,"pa_s" : "\n<p>"     ,"pa_e" : "</p>\n"
                ,"pr_s" : "\n<pre>"   ,"pr_e" : "</pre>\n"
                ,"ol_s" : "\n<ol>"    ,"ol_e" : "\n</ol>\n"
                ,"ul_s" : "\n<ul>"    ,"ul_e" : "\n</ul>\n"
                ,"li"   : "\n  <li>"  ,"lin"  : "\n  <li>"
                ,"a_s"  : "<a href='" ,"a_m"  : "'>" ,"a_e"  : "</a>"
                ,"b_s"  : "<b>"       ,"b_e"  : "</b>"
                ,"u_s"  : "<u>"       ,"u_e"  : "</u>"
                ,"i_s"  : "<i>"       ,"i_e"  : "</i>"
                ,"star": "*"
                ,"slash": "/"
                ,"under": "_"
                ,"lt"   : "&lt;"      ,"gt"   : "&gt;"
                }

  def __init__(self):
    pass

  def convert(self, s, tok):

    # put an in-page anchor and the section number at the start of each header
    for i, v in tok.table_of_contents.items():
      s = s.replace('{H-ID-'+str(i)+'}', '<a name="'+v['prefix']+'"></a>'+v['prefix'])

    # translate all the items from the tokens into the html 
    for k, v in self.translation.items():
      s = s.replace(tokenizer.tokens[k],v)

    # replace {TABLE-OF-CONTENTS} with the table of contents
    s = s.replace('{TABLE-OF-CONTENTS}',self.get_toc(tok))

    return s

  def get_toc(self, tok):
    t = '<ul class="table-of-contents">'
    for i, v in tok.table_of_contents.items():
      t += '<li style="font-size:140%; margin-left:'+str(33*v['level'])+'px;">'
      t += '<a href="#'+v['prefix']+'">'+v['prefix']+' '+v['label']+'</a>'
    t += '</ul>'
    return t



class restConverter():

  translation = {"nl"   : "\n"               ,"nlp"  : "\n"
                ,"h1_s" : "\n############\n" ,"h1_e" : "\n############\n"
                ,"h2_s" : "\n************\n" ,"h2_e" : "\n************\n"
                ,"h3_s" : "\n============\n" ,"h3_e" : "\n============\n"
                ,"pa_s" : "\n\n"             ,"pa_e" : ""
                ,"pr_s" : "\n``"             ,"pr_e" : "``"
                ,"ol_s" : "\n"               ,"ol_e" : ""
                ,"ul_s" : "\n"               ,"ul_e" : ""
                ,"li"   : "\n  * "           ,"lin"  : "\n  #. "
                ,"a_s"  : "`"                ,"a_m"  : " <" ,"a_e"  : ">`_"
                ,"b_s"  : "**"               ,"b_e"  : "**"
                ,"u_s"  : ""                 ,"u_e"  : ""
                ,"i_s"  : "*"                ,"i_e"  : "*"
                ,"star": "\*"
                ,"slash": "/"
                ,"under": "_"
                ,"lt"   : "<"                ,"gt"   : ">"
                }

  def __init__(self):
    pass
      
  def convert(self, s):
    t = tokenizer.tokens

    # rearrange links for rest
    s = re.sub(t['a_s']+'(.*?)'+t['a_m']+'(.*?)'+t['a_e'],
               t['a_s']+'\\2'+t['a_m']+'\\1'+t['a_e'],
               s)

    for k, v in self.translation.items():
      s = s.replace(tokenizer.tokens[k],v)
    return s



class markdownConverter():
  pass



if __name__ == "__main__":
  m = message()
  t = tokenizer()
  f = sys.argv[1]

  if not os.path.exists(f):
    m.die("File doesn't exist: "+f)

  with open(f) as l:
    lines = l.read()


  if len(sys.argv) == 1:
    m.die("Missing arguments.")
    
  elif len(sys.argv) == 2:
    s = t.tokenize(lines)
    c = htmlConverter()
    print c.convert(s, t)

  elif len(sys.argv) == 3 and sys.argv[2][-5:].lower() == ".html":
    s = t.tokenize(lines)
    c = htmlConverter()
    with open(sys.argv[2], 'w') as f:
      f.write(c.convert(s, t))

  elif len(sys.argv) == 3 and sys.argv[2][-4:].lower() == ".rst":
    s = t.tokenize(lines)
    c = restConverter()
    with open(sys.argv[2], 'w') as f:
      f.write(c.convert(s))



